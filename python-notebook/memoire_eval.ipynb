{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1f54e3",
   "metadata": {},
   "source": [
    "## 1. Load the RAG Mini-Wikipedia Dataset\n",
    "\n",
    "Loading the dataset that contains both questions and Wikipedia passages. This dataset will help us evaluate how well the retriever works by matching questions to the relevant passages from Wikipedia. Whu\n",
    "\n",
    "\n",
    "### Why this data?\n",
    "Dataset is opensource, and indexing time would be less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1901632",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 918 questions and 3200 passages.\n",
      "Example question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Answer: yes\n",
      "Ground-truth passage ID: 0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "questions_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\",  split=\"test\")\n",
    "passages_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\", split=\"passages\")\n",
    "\n",
    "print(f\"Loaded {len(questions_dataset)} questions and {len(passages_dataset)} passages.\")\n",
    "sample = questions_dataset[0]\n",
    "print(\"Example question:\", sample[\"question\"])\n",
    "print(\"Answer:\", sample[\"answer\"])\n",
    "print(\"Ground-truth passage ID:\", sample[\"id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd2b61",
   "metadata": {},
   "source": [
    "## 2. Configure Retriever API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870b589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "INDEX_ENDPOINT = \"http://localhost:3003//memoire/ingest/raw\"\n",
    "SEARCH_ENDPOINT = \"http://localhost:3003//memoire/search\"\n",
    "API_KEY = \"abc123\" \n",
    "\n",
    "K = 5  # for Precision@K, Recall@K, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a5c24a",
   "metadata": {},
   "source": [
    "## 3. Index the Passage Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72f611d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = []\n",
    "if 'id' in passages_dataset.column_names:\n",
    "    doc_ids = passages_dataset['id']\n",
    "else:\n",
    "    doc_ids = list(range(len(passages_dataset)))\n",
    "text_field = 'passage'\n",
    "doc_texts = passages_dataset[text_field]\n",
    "\n",
    "# Updatng by batches to avoid content too long error\n",
    "for doc_id, text in zip(doc_ids, doc_texts):\n",
    "    documents.append({\"documentID\": str(doc_id), \"content\": text})\n",
    "    if doc_id%32 == 0:\n",
    "        index_response = requests.post(INDEX_ENDPOINT, headers = {\"Authorization\": f\"Bearer {API_KEY}\"}, json = {\"documents\": documents})\n",
    "        documents = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714817ad",
   "metadata": {},
   "source": [
    "## 4. Retrieve Documents for Each Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9e8e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed retrieval for 918 queries.\n",
      "CPU times: user 2.26 s, sys: 204 ms, total: 2.46 s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "found_flags = []   \n",
    "ranks = []         \n",
    "latencies = []    \n",
    "\n",
    "for item in questions_dataset:\n",
    "    query = item[\"question\"]\n",
    "    true_id = item[\"id\"]\n",
    "    start_time = time.time()\n",
    "    response = requests.post(SEARCH_ENDPOINT, headers = {\"Authorization\": f\"Bearer {API_KEY}\"}, json={\"query\": query, \"maxResults\": K})\n",
    "    elapsed = time.time() - start_time\n",
    "    latencies.append(elapsed)\n",
    "    results = response.json()\n",
    "    top_docs = results[\"results\"] if \"results\" in results else results\n",
    "    retrieved_ids = [int(doc[\"documentID\"]) for doc in top_docs]\n",
    "\n",
    "    if true_id in retrieved_ids:\n",
    "        rank = retrieved_ids.index(true_id) + 1  \n",
    "        found_flags.append(1)\n",
    "        ranks.append(rank)\n",
    "    else:\n",
    "        found_flags.append(0)\n",
    "        ranks.append(None)\n",
    "\n",
    "total_queries = len(found_flags)\n",
    "print(f\"Completed retrieval for {total_queries} queries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070dc22",
   "metadata": {},
   "source": [
    "## 5. Compute Precision@K and Recall@K\n",
    "Precision@K measures how many of the top K results are relevant, while Recall@K measures how many relevant documents we found in the top K. In this case, we have a single relevant document for each question, so if it appears in the top K results, we count it as found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d536ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.000\n",
      "Recall@5: 0.001\n",
      "CPU times: user 380 μs, sys: 0 ns, total: 380 μs\n",
      "Wall time: 356 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "total_found = sum(found_flags)  \n",
    "N = len(found_flags)\n",
    "\n",
    "precision_at_k = total_found / (N * K)\n",
    "recall_at_k    = total_found / N\n",
    "\n",
    "print(f\"Precision@{K}: {precision_at_k:.3f}\")\n",
    "print(f\"Recall@{K}: {recall_at_k:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4a173",
   "metadata": {},
   "source": [
    "## 6. Compute Mean Reciprocal Rank (MRR)\n",
    "Mean Reciprocal Rank (MRR) tells us how early the relevant document appears in the top results. If the relevant document is ranked first, we get a high MRR score (1/1 = 1). The later it appears, the lower the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d345c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 μs, sys: 0 ns, total: 14 μs\n",
      "Wall time: 25.5 μs\n",
      "Mean Reciprocal Rank (MRR): 0.001\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Calculate reciprocal ranks for each query (0 if not found in top K)\n",
    "mrr_sum = 0.0\n",
    "for r in ranks:\n",
    "    if r is not None:\n",
    "        mrr_sum += 1.0 / r\n",
    "mrr = mrr_sum / N\n",
    "\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba219c33",
   "metadata": {},
   "source": [
    "## 7. Compute Normalized Discounted Cumulative Gain (nDCG) (similar to mmr)\n",
    "nDCG helps us measure how well the retriever ranks the relevant document. If the correct document appears early in the results, it contributes more to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4fb3ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@5: 0.001\n"
     ]
    }
   ],
   "source": [
    "ndcg_sum = 0.0\n",
    "for r in ranks:\n",
    "    if r is not None:\n",
    "        ndcg_sum += 1.0 / math.log2(r + 1)\n",
    "avg_ndcg = ndcg_sum / N\n",
    "\n",
    "print(f\"nDCG@{K}: {avg_ndcg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36a0ec",
   "metadata": {},
   "source": [
    "## 8. Analyze Query Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8346e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average latency per query: 106.11 ms\n",
      "Median latency: 104.61 ms\n",
      "95th percentile latency: 122.53 ms\n",
      "Max latency: 301.70 ms\n"
     ]
    }
   ],
   "source": [
    "latencies_ms = np.array(latencies) * 1000.0\n",
    "\n",
    "avg_latency    = np.mean(latencies_ms)\n",
    "median_latency = np.median(latencies_ms)\n",
    "p95_latency    = np.percentile(latencies_ms, 95)\n",
    "max_latency    = np.max(latencies_ms)\n",
    "\n",
    "print(f\"Average latency per query: {avg_latency:.2f} ms\")\n",
    "print(f\"Median latency: {median_latency:.2f} ms\")\n",
    "print(f\"95th percentile latency: {p95_latency:.2f} ms\")\n",
    "print(f\"Max latency: {max_latency:.2f} ms\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (venv)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
